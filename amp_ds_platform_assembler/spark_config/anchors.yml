anchors:
  - &proxy_properties
    spark.pie.driverEnv.KUBERNETES_SERVICE_HOST: kube-api.us-west-3q.aci.apple.com
    spark.pie.kubernetes.ipv6.enabled: false
    spark.pie.driverEnv.HTTPS_PROXY: http://dps.iso.apple.com:80
    spark.pie.driverEnv.HTTP_PROXY: http://dps.iso.apple.com:80
    spark.pie.driverEnv.NO_PROXY: localhost,apple.com
    spark.pie.executorEnv.NO_PROXY: localhost,apple.com
  - &whisper_properties
    spark.pie.kubernetes.data-platform-secrets.enabled: false
    spark.pie.kubernetes.sdr.appname: xoxhtzanezxptyqfvgjn
    spark.pie.kubernetes.whisper.enabled: true
    spark.pie.kubernetes.whisper.buckets: platform-spark-jobs
    spark.pie.kubernetes.whisper.namespace: amp-ds-platform-bi
  - &uc_properties
    hive.metastore.sasl.enabled: true
    spark.hadoop.hive.metastore.sasl.enabled: true
    spark.hadoop.hive.metastore.connect.retries: 10
    spark.hadoop.hive.metastore.execute.setugi: true
    hive.metastore.kerberos.principal: "UDSC/hms-dev001.udsc.datastudio.apple.com@PIE.APPLE.COM"
    hive.metastore.uris: "thrift://uc-hive-metastore-amp-prod.g.apple.com:8972"
    spark.sql.catalogImplementation: hive
    spark.ase.job.lineage.enabled: true
  - &sa_properties
    spark.pie.infer.hadoop.and.spark.user.from.kerberos.principal: true
    spark.pie.initialize.kerberos: true
    spark.pie.kerberos.keytab.path: "/mnt/app/.platform/whisper-secrets/app/HADOOP_KEYTAB"
    spark.pie.kerberos.principal.environment.variable: PLATFORM_KERBEROS_PRINCIPAL
  - &auto_size_properties
    spark.acs.autosize.enable: true
    spark.acs.autosize.policy: CPU,Memory,Disk
    spark.acs.autosize.executor.policy: similarruntime
    spark.acs.autosize.lookback.days: 60DAYS
    spark.acs.autosize.stats.rule: MAX
    spark.ase.kubernetes.autosize.revert.on.failure: true
  - &default_properties
    spark.driver.memory: 4g
    spark.executor.cores: 2
    spark.executor.disk: 5g
    spark.executor.memory: 4g
    spark.python.worker.memory: 4g
    spark.driver.maxResultSize: 0
    spark.pie.restart.on.failure: false
    spark.driver.extraJavaOptions: "-Dio.netty.tryReflectionSetAccessible=true -Djava.security.krb5.conf=/mnt/pie-config/generic/krb5.conf"
    spark.executor.extraJavaOptions: "-Dio.netty.tryReflectionSetAccessible=true -Djava.security.krb5.conf=/mnt/pie-config/generic/krb5.conf"
    spark.hadoop.fs.hdfs.impl: "org.apache.hadoop.hdfs.DistributedFileSystem"
    spark.hadoop.metastore.catalog.default: amp_prod_usmsc
    spark.kube.run.as.non-root.enabled: false
    spark.kubernetes.authenticate.driver.mounted.caCertFile: "/identity/trusted-root.pem"
    spark.kubernetes.executor.enablePollingWithResourceVersion: true
    spark.metrics.conf.*.sink.hubbleSink.hubble_channel_server_uri: "tcp://hubble-tcp-publish-pie-prod.apple.com:9898"
    spark.metrics.conf.*.sink.hubbleSink.hubble_publisher_tcp_server_url: "tcp://hubble-tcp-publish-pie-prod.apple.com:9898"
    spark.pie.base.image: fuji
    spark.pie.hadoop.config.service.additional.cluster.names: "usmsc23-meteor-adhoc,usmsc22-medusa-adhoc,usmsc06-median-sla,usmsc09-melody-sla,usmsc21-meson-sla"
    spark.pie.hadoop.config.service.cluster.name: usmsc22-medusa-adhoc
    spark.pie.hadoop.config.service.load.configs: "CORE,HDFS"
    spark.pie.include.hadoop.entitlements: true
    spark.pie.initialize.kerberos: true
    spark.pie.job.stop.on.deployment: false
    spark.pie.kubernetes.exposeLegacyComputeEnvVars: true
    spark.pie.profile.script.execution: true
    spark.pie.manualRun.default.workunit: usmsc04
    spark.pie.manualRun.max.concurrent.runs: 3
    spark.pie.pyFiles: "/mnt/app/pyspark-app/amp/sample/output.csv"
    spark.sql.adaptive.enabled: true
    spark.sql.catalog.spark_catalog: "org.apache.iceberg.spark.SparkSessionCatalog"
    spark.sql.catalog.spark_catalog.type: hive
    spark.sql.hive.metastorePartitionPruningFallbackOnException: true
    spark.sql.shuffle.partitions: 1600
    spark.sql.statistics.size.autoUpdate.enabled: true
    spark.pie.kubernetes.driver.priorityClassName: p1
    spark.pie.kubernetes.executor.priorityClassName: p1
    spark.kubernetes.driver.request.cores: 500m
    spark.kubernetes.driver.limit.cores: 2
    spark.kubernetes.executor.request.cores: 500m
    spark.kubernetes.executor.limit.cores: 4
    spark.pie.kubernetes.sidekick.container.limit.cores: 500m
    spark.pie.kubernetes.ephemeral.storage.volume.paths: '/mnt/app/logs,/mnt/switcher/app'
    spark.pie.logging.splunk.cluster.name: itunes_application
    spark.pie.logging.splunk.index.name: amp_ds_spark_jobs
    <<: [*whisper_properties, *uc_properties, *auto_size_properties, *sa_properties]